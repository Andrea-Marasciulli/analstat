% Giacomo Petrillo
% lezione di Punzi

% parte sulle sistematiche spostata dalla lezione 10 ottobre
\section{Sistematiche}

Quando non siamo sicuri sul modello da usare,
diciamo che abbiamo un'\emph{incertezza sistematica}.
Per trattare statisticamente l'incertezza sistematica,
bisogna fissare una famiglia di modelli.
L'unione di una famiglia di modelli è ancora un modello,
quindi a questo punto si può:
\begin{itemize}
	\item fare inferenza sul modello complessivo;
	\item fare una famiglia di inferenze.
\end{itemize}

\begin{example}
	Consideriamo un modello gaussiano di variabili $y_i$
	con media $\mu_i(\theta)$ funzione del parametro e varianze $\sigma_i^2$:
	\begin{equation*}
		p(\mathbf y;\theta)
		= \prod_i \frac1{\sqrt{2\pi}\sigma_i}
		\exp\left(-\frac12\left(\frac{y_i-\mu_i(\theta)}{\sigma_i}\right)^2\right).
	\end{equation*}
	Supponiamo di avere un dubbio su quali siano le $\sigma_i$.
	In generale il valore delle $\sigma_i$ influenzerà il risultato per $\theta$.
	Allora possiamo considerare le $\sigma_i$ come un parametro
	e fare inferenza sia su $\theta$ che sulle varianze:
	\begin{equation*}
		p(\mathbf y;\theta) \rightarrow p(\mathbf y;\theta,\boldsymbol\sigma).
	\end{equation*}
	Un modo più ``agnostico'' di risolvere la questione è tenere le $\sigma_i$ fissate
	e dire quale risultato si ottiene per $\theta$ al variare delle $\sigma_i$
	tra alcuni valori di interesse.
\end{example}

\marginpar{Aggiungere questo esempio:
misuro con un voltmetro una tensione,
ho un macchinario il cui comportamento dipende da questa tensione.
Faccio delle misure e uso un modello complicato per il macchinario,
per trattare statisticamente la misura di tensione devo mettere
un termine es. gaussiano per essa e aggiungerla come nuisance parameter,
però poi se faccio il fit vedo che la stima della tensione viene molto
diversa da quella misurata con il voltmetro.
Poiché mi fido più della misura diretta del voltmetro
che del modello complicato con cui sto fittando,
elimino la tensione dalla trattazione statistica,
e vedo come varia il risultato del fit variando di $\pm 1\sigma$ la tensione che impongo.}

\section{Likelihood gaussiana}

Consideriamo la gaussiana con media $\mu$ e deviazione standard $\sigma$:
\begin{equation*}
	g(x;\mu,\sigma) \is \frac1{\sqrt{2\pi}\sigma} e^{-\frac12 \left( \frac{x-\mu}\sigma \right)^2}.
\end{equation*}
Calcoliamo la likelihood per $\mu$ con due misure:
\begin{align*}
	\mathcal L(\mu)
	&= g(x_1;\mu,\sigma) g(x_2;\mu,\sigma) = \\
	&= \frac1{2\pi\sigma^2}
	e^{\left( \frac{x_1-x_2}{2\sigma} \right)^2}
	e^{-\frac2{2\sigma^2} \left( \frac{x_1+x_2}2 - \mu \right)^2} = \\
	&= f(x_1,x_2) g \big( \mu;\bar x,\sigma/\sqrt2 \big).
\end{align*}
Dunque la likelihood per $\mu$ è gaussiana.
Notiamo che la distribuzione di $\bar x$ ha la stessa forma $g \big( \bar x;\mu,\sigma/\sqrt2 \big)$
e che questo non vale in generale, si pensi ad esempio alla distribuzione uniforme
in cui la somma di due variabili è triangolare mentre la likelihood per la media della distribuzione è un'iperbole troncata.

\section{Sufficienza}

\begin{definition}[Sufficienza]
	\label{th:suff}
	Consideriamo un modello per $x$ con parametro $\theta$ e una statistica $s$.
	Si dice che $s$ è \emph{sufficiente} per $\theta$ se la distribuzione di $x$ si fattorizza come
	$p(x;\theta) = f(x) p(s(x);\theta)$ con $f\ge0$.
\end{definition}

Il senso della definizione di sufficienza
è che la likelihood calcolata con una statistica sufficiente
è equivalente a quella calcolata direttamente con la variabile,
cioè cambia di una costante moltiplicativa positiva:
\begin{align*}
	\mathcal L_{x_0}(\theta)
	&= p(x_0;\theta) = f(x_0) p(s(x_0);\theta) \propto \\
	\propto \mathcal L_{s(x_0)}(\theta)
	&= p(s(x_0);\theta),
\end{align*}
quindi per scrivere la likelihood è sufficiente conoscere $s_0 \is s(x_0)$.

\begin{theorem}
	Se $s(x)$ è sufficiente per $\theta$,
	la probabilità condizionata $p(x|s)$ non dipende da $\theta$.
\end{theorem}

\begin{proof}
	Basta calcolarla:
	\begin{align*}
		p(x|s_0;\theta)
		&= \frac {p(x,s_0;\theta)} {p(s_0;\theta)} = \\
		&= \frac {p(x;\theta) \delta(s_0-s(x))} {p(s_0;\theta)} = \\
		&= \frac {f(x) p(s(x);\theta) \delta(s_0-s(x))} {p(s_0;\theta)} = \\
		&= f(x) \delta(s_0-s(x)) = \\
		&= f(x) \sum_{x'\in s^{-1}(s_0)} \frac {\delta(x-x')} {|s'(x')|}. \qedhere
	\end{align*}
\end{proof}

\begin{definition}
	Una statistica sufficiente $s$ è \emph{minimale} se
	\begin{equation*}
		\forall \sigma \text{ statistica sufficiente}\ \exists f\ \forall x:
		s(x) = f(\sigma(x)).
	\end{equation*}
\end{definition}
\noindent Nota: non è detto che la statistica sufficiente minimale sia unica.

\begin{theorem}
	\label{th:suffatt}
	Se, per una qualche funzione $g$, la probabilità di $x$ si fattorizza come
	\begin{equation*}
		p(x;\mu) = f(x)\cdot g(s(x),\theta)
	\end{equation*}
	con $f\ge 0$, allora $s$ è sufficiente.
\end{theorem}

\begin{proof}
	Calcoliamo la probabilità di $s(x)$:
	\begin{align*}
		p(s(x);\theta)
		&= \sum_{x':s(x')=s(x)} \frac{p(x';\theta)}{J(x')} = \\
		&= \sum_{x':s(x')=s(x)} \frac{f(x') g(s(x'),\theta)}{J(x')} = \\
		&= \left( \sum_{x':s(x')=s(x)} \frac{f(x')}{J(x')} \right) g(s(x),\theta),
	\end{align*}
	dove $J$ è il jacobiano di $s$.
	Se la somma su $x'$ è nulla,
	essendo $f\ge 0$ devono essere nulli tutti i termini quindi in particolare $f(x)$.
	Allora posso scrivere
	\begin{align*}
		p(x;\theta)
		&= F(x) p(s(x);\theta) \\
		\intertext{dove}
		F(x)
		&= \begin{cases}
			\displaystyle\frac{f(x)}{\sum_{x':s(x')=s(x)} \frac{f(x')}{J(x')}}
			& \text{se }\sum_{x':s(x')=s(x)} \frac{f(x')}{J(x')} \neq 0 \\
			0 & \text{altrimenti.}
		\end{cases} \qedhere
	\end{align*}
\end{proof}

\begin{fact}[Teorema di Darmois]
	\label{th:darmois}
	Sia $x$ variabile reale e $\fundef[\mathbf s_N]{\R^N}{\R^d}$ statistica sufficiente per $\theta$ di $N$ estrazioni di $x$ per ogni $N$.
	Se il supporto di $p(x;\theta)$ non dipende da $\theta$, allora necessariamente
	\begin{equation*}
		p(x;\theta) =
		\exp \left( \sum_{k=1}^d \alpha_k(x)a_k(\theta) + \beta(x) + \gamma(\theta) \right)
	\end{equation*}
	e la statistica sufficiente è, a meno di una funzione biunivoca,
	\begin{equation}
		\label{eq:statdarmois}
		s_{N,k}(\mathbf x) = \sum_{i=1}^{N} \alpha_k(x_i);
	\end{equation}
	le pdf in questa forma si chiamano \emph{famiglia esponenziale}.
	Viceversa, se la pdf è della famiglia esponenziale, la statistica \eqref{eq:statdarmois} è sufficiente.
\end{fact}

\begin{exercise}
	Dimostrare la seconda freccia del teorema di Darmois.
\end{exercise}

\begin{solution}
	Calcoliamo la pdf delle estrazioni:
	\begin{align*}
		p(\mathbf x;\mu)
		&= \prod_{k=1}^N p(x_k;\mu) = \\
		&= \exp \left( \sum_{k=1}^N \sum_{i=1}^d \alpha_i(x_k)a_i(\mu)
		+ \sum_{k=1}^N \beta(x_k) + N\gamma(\mu) \right) = \\
		&= \exp \left( \sum_{k=1}^N \beta(x_k) \right)
		\cdot \exp \left( \sum_{i=1}^d a_i(\mu)s_i(\mathbf x) + N\gamma(\mu) \right) = \\
		&= f(\mathbf x) \cdot g(\mathbf s(\mathbf x);\mu), \quad f\ge0.
	\end{align*}
	Il \autoref{th:suffatt} conclude.
	Notiamo che la dimostrazione vale anche se $x$ non è una variabile reale.
\end{solution}
