% Giacomo Petrillo
% lezione di Francavilla

\begin{exercise}
	\label{th:troncexp}
	Consideriamo un'esponenziale troncata a destra:
	\begin{equation*}
		p(t;\lambda)
		= \frac{\lambda e^{-\lambda t}}{1 - e^{-\lambda T}},
		\quad t \in (0,T).
	\end{equation*}
	Calcolare l'informazione di Fisher per $\lambda$.
\end{exercise}

\begin{solution*}
	\begin{align*}
		\log p(t;\lambda)
		&= \log\lambda - \lambda t - \log(1-e^{-\lambda T}) \\
		\frac{\partial}{\partial\lambda} \log p(t;\lambda)
		&= \frac1\lambda - t - \frac{-(-T)e^{-\lambda T}}{1-e^{-\lambda T}} = \\
		&= \frac1\lambda - t - \frac{T}{e^{\lambda T}-1} \\
		-\frac{\partial^2}{\partial\lambda^2} \log p(t;\lambda)
		&= \frac1{\lambda^2} - \frac{T^2e^{\lambda T}}{(e^{\lambda T}-1)^2}.
	\end{align*}
	Notiamo che per $\lambda T\to0$ l'informazione va a zero mentre per $\lambda T\to\infty$
	riotteniamo l'informazione dell'esponenziale.
	Calcoliamo la variazione relativa rispetto a quest'ultima:
	\begin{equation*}
		\frac{I_T(\lambda)}{I_\infty(\lambda)}
		= \lambda^2 I_T(\lambda)
		= 1 - \frac {(\lambda T)^2e^{\lambda T}} {(e^{\lambda T}-1)^2}.
	\end{equation*}
	Ad esempio, per $\lambda T=5$ l'informazione cala del \SI{17}\percent.
	Se immaginiamo di perdere bernouillianamente una frazione
	\begin{equation*}
		\epsilon
		= \int_T^\infty \de t\, \lambda e^{-\lambda t}
		= e^{-\lambda T}
	\end{equation*}
	delle misure di un'esponenziale,
	la variazione relativa dell'informazione di Fisher è~$-\epsilon$
	che per $\lambda T=5$ vale \SI{0.7}\percent.
	\marginpar{Questa cosa non è chiarissima perché se immagino di misurare il tempo tra un evento e l'altro e non osservo un evento con probabilità $\epsilon$ (vedi \autoref{th:salvexp}) l'informazione di Fisher non cambia.
	Quello che qui si intende è come se facessi le misure e poi ne buttassi via alcune (tipo trigger).}
\end{solution*}

\begin{exercise}
	Calcolare l'informazione di Fisher di un'esponenziale troncata a sinistra.
\end{exercise}

\begin{solution}
	Scriviamo la pdf:
	\begin{align*}
		p(t;\lambda)
		&= \frac {\lambda e^{-\lambda t}} {\int_T^\infty \de t\, \lambda e^{-\lambda t}} = \\
		&= \lambda e^{-\lambda (t-T)}, \quad t\in(T,\infty).
	\end{align*}
	Com'era intuitivo è un'esponenziale traslata quindi l'informazione di Fisher è ancora~$1/\lambda^2$.
\end{solution}

\begin{exercise}
	\label{th:avgsuffgauss}
	La media aritmetica è una statistica sufficiente per la media della gaussiana?
\end{exercise}

\begin{solution*}
	Sì:
	\begin{align*}
		p(\mathbf x;\mu)
		&= \prod_{i=1}^N \frac1{\sqrt{2\pi}\sigma} 
		\exp \left( -\frac12 \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&\left[ \sum_{i=1}^N (x_i-\mu)^2
		= \sum_{i=1}^N x_i^2 + N\mu^2 - 2\mu N\bar x \right] \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \frac{x_i^2}{\sigma^2} \right)
		\cdot \exp \left( -\frac{N\mu^2-2\mu N\bar x}{2\sigma^2} \right) = \\
		&= f(\mathbf x) \cdot g(\bar x,\mu).
	\end{align*}
\end{solution*}

\begin{exercise}
	Lo scarto quadratico medio è una statistica sufficiente per la deviazione standard della gaussiana?
\end{exercise}

\begin{solution*}
	È ancor più immediato dell'\autoref{th:avgsuffgauss}:
	\begin{align*}
		\hat\sigma^2(\mathbf x)
		&= \frac1N \sum_{i=1}^N (x_i-\mu)^2 \\
		p(\mathbf x;\sigma)
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \frac{N\hat\sigma^2}{\sigma^2} \right) = \\
		&= g(\hat\sigma,\sigma).
	\end{align*}
\end{solution*}

\begin{exercise}
	Verificare se la media aritmetica è sufficiente per la media delle distribuzioni
	binomiale,
	poissoniana,
	esponenziale.
\end{exercise}

\begin{solution}
	È sufficiente per tutte e tre.
	\begin{description}
		\item[Binomiale]
		La media è $np$, fissiamo $n$ e usiamo $p$ come parametro:
		\begin{align*}
			p(\mathbf k;p)
			&= \prod_{i=1}^N \binom{n}{k_i} p^{k_i} (1-p)^{n-k_i} = \\
			&= \left( \prod_{i=1}^N \binom{n}{k_i} \right) \cdot
			p^{N\bar k} (1-p)^{Nn-N\bar k}.
		\end{align*}
		\item[Poissoniana]
		\begin{align*}
			p(\mathbf k;\mu)
			&= \prod_{i=1}^N \frac{\mu^{k_i}}{k_i!}e^{-\mu} = \\
			&= \left( \prod_{i=1}^N \frac1{k_i!} \right) \cdot \mu^{N\bar k} e^{-N\mu}.
		\end{align*}
		\item[Esponenziale]
		La media è $1/\lambda$, usiamo $\lambda$ come parametro:
		\begin{align*}
			p(\mathbf t;\lambda)
			&= \prod_{i=1}^N \lambda e^{-\lambda t_i} = \\
			&= \lambda^N e^{-N\lambda\bar t}.
		\end{align*}
	\end{description}
\end{solution}
