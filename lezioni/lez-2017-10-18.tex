% Giacomo Petrillo
% lezione di Francavilla

\begin{exercise}
	\label{th:troncexp}
	Consideriamo un'esponenziale troncata a destra:
	\begin{equation*}
		p(t;\lambda)
		= \frac{\lambda e^{-\lambda t}}{1 - e^{-\lambda T}},
		\quad t \in (0,T).
	\end{equation*}
	Calcolare l'informazione di Fisher per $\lambda$.
\end{exercise}

\begin{solution*}
	\begin{align*}
		\log p(t;\lambda)
		&= \log\lambda - \lambda t - \log(1-e^{-\lambda T}) \\
		\frac{\partial}{\partial\lambda} \log p(t;\lambda)
		&= \frac1\lambda - t - \frac{-(-T)e^{-\lambda T}}{1-e^{-\lambda T}} = \\
		&= \frac1\lambda - t - \frac{T}{e^{\lambda T}-1} \\
		-\frac{\partial^2}{\partial\lambda^2} \log p(t;\lambda)
		&= \frac1{\lambda^2} - \frac{T^2e^{\lambda T}}{(e^{\lambda T}-1)^2}.
	\end{align*}
	Notiamo che per $\lambda T\to0$ l'informazione va a zero mentre per $\lambda T\to\infty$
	riotteniamo l'informazione dell'esponenziale.
	Calcoliamo la variazione relativa rispetto a quest'ultima:
	\begin{equation*}
		\frac{I_T(\lambda)}{I_\infty(\lambda)}
		= \lambda^2 I_T(\lambda)
		= 1 - \frac {(\lambda T)^2e^{\lambda T}} {(e^{\lambda T}-1)^2}.
	\end{equation*}
	Ad esempio, per $\lambda T=5$ l'informazione cala del \SI{17}\percent.
	Se immaginiamo di perdere bernouillianamente una frazione
	\begin{equation*}
		\epsilon
		= \int_T^\infty \de t\, \lambda e^{-\lambda t}
		= e^{-\lambda T}
	\end{equation*}
	delle misure di un'esponenziale,
	la variazione relativa dell'informazione di Fisher è~$-\epsilon$
	che per $\lambda T=5$ vale \SI{0.7}\percent.
	\marginpar{Questa cosa non è chiarissima perché se immagino di misurare il tempo tra un evento e l'altro e non osservo un evento con probabilità $\epsilon$ (vedi \autoref{th:salvexp}) l'informazione di Fisher non cambia.
	Quello che qui si intende è come se facessi le misure e poi ne buttassi via alcune (tipo trigger).}
\end{solution*}

\begin{exercise}
	Calcolare l'informazione di Fisher di un'esponenziale troncata a sinistra.
\end{exercise}

\begin{solution}
	Scriviamo la pdf:
	\begin{align*}
		p(t;\lambda)
		&= \frac {\lambda e^{-\lambda t}} {\int_T^\infty \de t\, \lambda e^{-\lambda t}} = \\
		&= \lambda e^{-\lambda (t-T)}, \quad t\in(T,\infty).
	\end{align*}
	Com'era intuitivo è un'esponenziale traslata quindi l'informazione di Fisher è ancora~$1/\lambda^2$.
\end{solution}

\begin{exercise}
	\label{th:avgsuffgauss}
	La media aritmetica è una statistica sufficiente per la media della gaussiana?
\end{exercise}

\begin{solution*}
	Sì:
	\begin{align*}
		p(\mathbf x;\mu)
		&= \prod_{i=1}^N \frac1{\sqrt{2\pi}\sigma} 
		\exp \left( -\frac12 \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&\left[ \sum_{i=1}^N (x_i-\mu)^2
		= \sum_{i=1}^N x_i^2 + N\mu^2 - 2\mu N\bar x \right] \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \frac{x_i^2}{\sigma^2} \right)
		\cdot \exp \left( -\frac{N\mu^2-2\mu N\bar x}{2\sigma^2} \right) = \\
		&= f(\mathbf x) \cdot g(\bar x,\mu).
	\end{align*}
\end{solution*}

\begin{exercise}
	Lo scarto quadratico medio è una statistica sufficiente per la deviazione standard della gaussiana?
\end{exercise}

\begin{solution*}
	È ancor più immediato dell'\autoref{th:avgsuffgauss}:
	\begin{align*}
		\hat\sigma^2(\mathbf x)
		&\is \frac1N \sum_{i=1}^N (x_i-\mu)^2 \\
		p(\mathbf x;\sigma)
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \frac{N\hat\sigma^2}{\sigma^2} \right) = \\
		&= g(\hat\sigma,\sigma).
	\end{align*}
\end{solution*}

\begin{exercise}
	Verificare se la media aritmetica è sufficiente per la media delle distribuzioni
	binomiale,
	poissoniana,
	esponenziale.
\end{exercise}

\begin{solution}
	È sufficiente per tutte e tre.
	\begin{description}
		\item[Binomiale]
		La media è $np$, fissiamo $n$ e usiamo $p$ come parametro:
		\begin{align*}
			p(\mathbf k;p)
			&= \prod_{i=1}^N \binom{n}{k_i} p^{k_i} (1-p)^{n-k_i} = \\
			&= \left( \prod_{i=1}^N \binom{n}{k_i} \right) \cdot
			p^{N\bar k} (1-p)^{Nn-N\bar k}.
		\end{align*}
		\item[Poissoniana]
		\begin{align*}
			p(\mathbf k;\mu)
			&= \prod_{i=1}^N \frac{\mu^{k_i}}{k_i!}e^{-\mu} = \\
			&= \left( \prod_{i=1}^N \frac1{k_i!} \right) \cdot \mu^{N\bar k} e^{-N\mu}.
		\end{align*}
		\item[Esponenziale]
		La media è $1/\lambda$, usiamo $\lambda$ come parametro:
		\begin{align*}
			p(\mathbf t;\lambda)
			&= \prod_{i=1}^N \lambda e^{-\lambda t_i} = \\
			&= \lambda^N e^{-N\lambda\bar t}.
		\end{align*}
	\end{description}
\end{solution}

\begin{exercise}
	Applicare il teorema di Darmois (\autoref{th:darmois}) alla gaussiana separatamente per la media e per la varianza.
\end{exercise}

\begin{solution*}
	Dobbiamo scrivere la pdf nella forma:
	\begin{equation*}
		p(x;\theta)
		= \exp \left( \sum_{k=1}^d \alpha_k(x)a_k(\theta) + \beta(x) + \gamma(\theta) \right).
	\end{equation*}
	Procediamo per la media:
	\begin{align*}
		p(x;\mu)
		&= \frac1{\sqrt{2\pi}\sigma}
		\exp\left( -\frac12 \left(\frac{x-\mu}\sigma\right)^2 \right) = \\
		&= \frac1{\sqrt{2\pi}\sigma}
		\exp\left( \frac{-x^2-\mu^2+2\mu x}{2\sigma^2} \right) = \\
		&= \exp \left( x\frac\mu{\sigma^2} - \frac{x^2}{2\sigma^2}
		 - \frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right),
	\end{align*}
	quindi le varie funzioni del teorema di Darmois sono:
	\begin{align*}
		\alpha(x) &= x \hspace{5em} (d = 1)\\
		a(\mu) &= \frac{\mu}{\sigma^2} \\
		\beta(x) &= - \frac{x^2}{2\sigma^2} \\
		\gamma(\mu) &= - \frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma),
	\end{align*}
	da cui ricaviamo la statistica sufficiente:
	\begin{equation*}
		s(\mathbf x)
		= \sum_{i=1}^N \alpha(x_i)
		= N\bar x.
	\end{equation*}
	Come già sapevamo, la media aritmetica è sufficiente,
	quindi se a meno di cambi di variabile esiste una sola statistica sufficiente,
	deve essere la media aritmetica.
	
	Vediamo per la varianza:
	\begin{align*}
		p(x;\sigma)
		&= \exp \left( (2x\mu-x^2)\frac1{2\sigma^2}
		- \frac{\mu^2}{2\sigma^2}
		-\log(\sqrt{2\pi}\sigma) \right) \\
		\implies s(\mathbf x)
		&= \sum_{i=1}^N (2x_i\mu-x_i^2) = \\
		&= -\sum_{i=1}^N (x_i-\mu)^2 + \mu^2 = \\
		&= \mu^2 - N\hat\sigma^2.
	\end{align*}
\end{solution*}

\begin{exercise}
	Trovare una statistica sufficiente simultaneamente per media e varianza della gaussiana.
\end{exercise}

\begin{solution}
	Usiamo il teorema di Darmois:
	\begin{align*}
		p(x;\mu,\sigma)
		&= \exp \left( \frac{-x^2-\mu^2+2\mu x}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right) = \\
		&= \exp \left( -x^2\frac1{2\sigma^2}
		+ x\frac\mu{\sigma^2}
		-\frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right);
	\end{align*}
	le varie funzioni sono:
	\begin{align*}
		a_{1,2}(x) &= x^2, x \hspace{6em} (d=2)\\
		\alpha_{1,2}(\mu,\sigma) &= -\frac1{2\sigma^2}, \frac\mu{\sigma^2} \\
		\beta(x) &= 0 \\
		\gamma(\mu,\sigma) &= -\frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma)
	\end{align*}
	quindi la statistica sufficiente è
	\begin{equation*}
		\mathbf s(\mathbf x) = \sum_{i=1}^N \begin{pmatrix} x_i^2 \\ x_i \end{pmatrix}
	\end{equation*}
	dove la somma si intende componente per componente.
	Questa statistica ha l'utile proprietà che può essere calcolata leggendo una $x$ alla volta e aggiungendo $(x^2,x)$ alla somma parziale, quindi non serve salvare tutte le misure.
	Notiamo che con un cambio di variabili possiamo scrivere la statistica in un modo più familiare:
	\begin{align*}
		\mathbf S(\mathbf s)
		&= \begin{pmatrix}
			s_2 / N \\
			s_1 / N - s_2^2 / N^2
		\end{pmatrix} \\
		S_1(\mathbf s(\mathbf x)) 
		&= \frac{\sum_ix_i}N = \bar x \\
		S_2(\mathbf s(\mathbf x))
		&= \frac{\sum_ix_i^2}N - \frac{\big(\sum_ix_i\big)^2}{N^2} = \\
		&= \frac{\sum_ix_i^2}N - 2\frac{\big(\sum_ix_i\big)^2}{N^2} + \frac{\big(\sum_ix_i\big)^2}{N^2} = \\
		&= \frac1N\sum_ix_i^2 - \frac1N2\bar x\sum_ix_i + \bar x^2 = \\
		&= \frac1N\sum_i(x_i-\bar x)^2.
	\end{align*}
\end{solution}

\begin{exercise}
	Applicare Darmois a binomiale, poissoniana, esponenziale.
\end{exercise}
