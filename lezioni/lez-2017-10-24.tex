% Giacomo Petrillo
% lezione di Punzi

\section{Stima puntuale}

\begin{definition}[Stima]
	Dato un modello con parametro $\theta$,
	una \emph{stima di $\theta$} è una statistica a valori nello spazio di $\theta$.
\end{definition}

\begin{definition}[Consistenza]
	Dato un modello con parametro $\theta$ per $N$ estrazioni di $x$
	e una stima $\hat\theta_N$ di $\theta$,
	la stima è \emph{consistente}
	se per $N\to\infty$ converge debolmente al \emph{valore vero},
	cioè il valore del parametro in cui calcoliamo la probabilità:
	\begin{equation*}
		\forall\epsilon\, \lim_{N\to\infty} P(|\hat\theta_N(\mathbf x)-\theta| > \epsilon;\theta) = 0.
	\end{equation*}
\end{definition}

\begin{definition}[Bias]
	Dato un modello con parametro $\theta$ e una stima $\hat\theta$ di $\theta$,
	il \emph{bias} è la differenza tra la media della stima e il valore vero:
	\begin{equation*}
		b(\theta) \is E[\hat\theta;\theta] - \theta.
	\end{equation*}
	Una stima si dice \emph{corretta} se il bias è nullo;
	\emph{asintoticamente corretta} se il modello è per $N$ estrazioni
	e nel limite $N\to\infty$ il bias tende a zero.
\end{definition}

\subsection{Efficienza}

\begin{theorem}[Disuguaglianza di Cramer-Rao]
	\label{th:cramerrao}
	Consideriamo un modello con parametro $\theta\in\R$.
	Sia $\hat\theta$ una stima con bias~$b$
	la cui pdf ha supporto che non dipende da $\theta$.
	Allora c'è un limite inferiore alla varianza della stima:
	\begin{equation*}
		\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_{\hat\theta}(\theta)},
	\end{equation*}
	dove $I$ è l'informazione di Fisher.
\end{theorem}

\begin{proof}
	Useremo il logaritmo della pdf quindi adottiamo la convenzione di restringere il dominio al supporto.
	\begin{align*}
		L &\is p(\hat\theta;\theta)
	\end{align*}
	\begin{align*}
		\int \de\hat\theta\,
		L \cdot \hat\theta \pdv{\log L}\theta 
		&= \int \de\hat\theta\,
		\pdv L\theta \hat\theta = \\
		&= \int \de\hat\theta\,
		\pdv{}\theta (\hat\theta L) = \\
		&= \pdv{}\theta \int \de\hat\theta\, \hat\theta L = \\
		&= \pdv{}\theta E[\hat\theta;\theta] = \\
		&= \pdv{}\theta (\theta + b(\theta)) = \\
		&= 1 + \pdv b\theta
	\end{align*}
	\begin{align*}
		\int \de\hat\theta\,
		L \cdot E[\hat\theta;\theta] \pdv{\log L}\theta
		&= E[\hat\theta;\theta] \int \de\hat\theta\, \pdv L\theta = \\
		&= E[\hat\theta;\theta] \pdv{}\theta \int \de\hat\theta\, L = \\
		&= 0
	\end{align*}
	\begin{align*}
		E \left[ (\hat\theta - E[\hat\theta;\theta]) \pdv{\log L}\theta;\theta \right]
		&= \int \de\hat\theta\, L \cdot
		(\hat\theta - E[\hat\theta;\theta]) \pdv{\log L}\theta = \\
		&= 1 + \pdv b\theta.
	\end{align*}
	Scriviamo la disuguaglianza di Schwartz in termini di valori attesi
	e applichiamola a quello appena calcolato:
	\begin{align*}
		E[fg]^2 &\le E[f^2]E[g^2] \\
		\left( 1 + \pdv b\theta \right)^2
		&= E \left[ (\hat\theta - E[\hat\theta;\theta]) \pdv{\log L}\theta;\theta \right]^2 \le \\
		&\le E[(\hat\theta - E[\hat\theta;\theta])^2;\theta]
		\cdot E \left[ \left( \pdv{\log L}\theta \right)^2;\theta \right] = \\
		&= \var[\hat\theta;\theta] I_{\hat\theta}(\theta). \qedhere
	\end{align*}
\end{proof}

Per il \autoref{th:fishstat} l'informazione di Fisher di $x$ è maggiore o uguale a quella di $\hat\theta$,
quindi si ha il corollario che
\begin{equation*}
	\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)};
\end{equation*}
nel caso di bias che dipende poco dal parametro ($|\partial b/\partial\theta| \ll 1$)
questo limite inferiore, detto \emph{minimum variance bound}, non dipende dallo stimatore.

Nel caso di informazione di Fisher proporzionale al numero di estrazioni otteniamo
\begin{equation*}
	\var[\hat\theta] \ge O \left(\frac1N\right),
\end{equation*}
questa è la forma generale dell'osservazione che comunemente
<<l'errore cala con la radice del numero di misure>>.

\begin{definition}[Efficienza]
	Con la notazione del \autoref{th:cramerrao} e con $x$ variabile del modello,
	l'\emph{efficienza di $\hat\theta$} è
	\begin{equation*}
		\frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)\var[\hat\theta;\theta]},
	\end{equation*}
	che nelle ipotesi del teorema è minore o uguale a~1.
\end{definition}

\begin{theorem}
	\label{th:suffeff}
	Con la notazione del \autoref{th:cramerrao} e con $x$ variabile del modello,
	se $\hat\theta$ è sufficiente per $\theta$ allora ha efficienza unitaria.
	\marginpar{Il Del Prete a pagina 177 (in fondo)
	dice che efficiente $\rightarrow$ sufficiente,
	ma anche che non vale il viceversa. \\
	\emph{Sì, e due pagine dopo <<stressa>> che la likelihood è il posteriore!}}
\end{theorem}

\begin{proof}
	Per il \autoref{th:darmois} (teorema di Darmois),
	la pdf di $x$ si scrive come
	\begin{equation*}
		p(x;\theta)
		= \exp \left( \hat\theta(x)a(\theta) + \beta(x) + \gamma(\theta) \right).
	\end{equation*}
	Per l'ipotesi di sufficienza, la pdf di $x$ si fattorizza con quella di $\hat\theta$:
	\begin{equation*}
		p(x;\theta)
		= f(x) p(\hat\theta(x);\theta), \quad f > 0,
	\end{equation*}
	quindi, per qualsiasi $x\in\hat\theta^{-1}(\hat\theta_0)$, si ha
	\begin{align*}
		p(\hat\theta_0;\theta)
		&= \frac{p(x;\theta)}{f(x)} = \\
		&= \exp \left( \hat\theta_0 a(\theta) + \beta(x) + \gamma(\theta) - \log f(x) \right).
	\end{align*}
	Poiché l'espressione dipende solo da $\theta_0$ (e $\theta$),
	possiamo definire $B(\hat\theta_0)\is\beta(x)-\log f(x)$,
	quindi infine
	\begin{equation*}
		p(\hat\theta;\theta)
		= \exp \left( \hat\theta a(\theta) + B(\hat\theta) + \gamma(\theta) \right).
	\end{equation*}
	Sia $g(\theta)\is a'(\theta)$.
	Cerchiamo una relazione tra $\gamma$ e $g$:
	\marginpar{Fare al contrario partendo dal fatto che dobbiamo saturare Schwartz
	e mostrare che $a'(\theta)E[\hat\theta;\theta]=-\gamma'(\theta)$.}%
	\begin{align*}
		\gamma(\theta)
		&= \int_{\theta_0}^\theta \de\vartheta\, \gamma'(\vartheta) + \gamma(\theta_0) = \\
		&= \int_{\theta_0}^\theta \de\vartheta\,
		\gamma'(\vartheta) e^{-\gamma(\vartheta)} e^{\gamma(\vartheta)}
		+ \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta\,
		\pdv{}{\vartheta}\big(e^{-\gamma(\vartheta)}\big) e^{\gamma(\vartheta)}
		+ \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta\,
		\pdv{}{\vartheta} \left( \int\de\hat\theta\, p(\hat\theta;\vartheta) e^{-\gamma(\vartheta)} \right)
		e^{\gamma(\vartheta)} + \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta \int\de\hat\theta\,
		\pdv{}{\vartheta} \big( e^{\hat\theta a(\vartheta) + B(\hat\theta)} \big) e^{\gamma(\vartheta)}
		+ \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta \int\de\hat\theta\,
		\hat\theta a'(\vartheta) e^{\hat\theta a(\vartheta) + B(\hat\theta) + \gamma(\vartheta)}
		+ \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta\, a'(\vartheta)
		\int\de\hat\theta\, p(\hat\theta;\vartheta) \hat\theta
		+ \gamma(\theta_0) = \\
		&= - \int_{\theta_0}^\theta \de\vartheta\, g(\vartheta) E[\hat\theta;\vartheta]
		+ \gamma(\theta_0),
	\end{align*}
	e inoltre abbiamo per definizione:
	\begin{equation*}
		a(\theta)
		= \int_{\theta_0}^\theta \de\vartheta\, g(\vartheta) + a(\theta_0),
	\end{equation*}
	quindi:
	\begin{align*}
		\log p(\hat\theta;\theta)
		&= \left( \int_{\theta_0}^\theta \de\vartheta\, g(\vartheta) + a(\theta_0) \right) \hat\theta
		+ B(\hat\theta) + {}\\
		&\phantom{{}={}}- \int_{\theta_0}^\theta \de\vartheta\, g(\vartheta) E[\hat\theta;\vartheta] + \gamma(\theta_0) = \\
		&= \int_{\theta_0}^\theta \de\vartheta\, g(\vartheta) \big(\hat\theta-E[\hat\theta;\vartheta]\big)
		+ a(\theta_0)\hat\theta + B(\hat\theta) + \gamma(\theta_0).
	\end{align*}
	Deriviamo rispetto al parametro:
	\begin{equation*}
		\pdv{}{\theta} \log p(\hat\theta;\theta)
		= g(\theta) \cdot \big(\hat\theta-E[\hat\theta;\theta]\big).
	\end{equation*}
	Riguardando la dimostrazione del \autoref{th:cramerrao},
	vediamo che la disuguaglianza di Schwartz è saturata perché le due funzioni sono proporzionali.
	Poiché $\hat\theta$ è sufficiente, $I_x(\theta)=I_{\hat\theta}(\theta)$,
	quindi l'efficienza è unitaria.
\end{proof}

Questo teorema non dice nulla sul bias di $\hat\theta$,
però ha bias nullo se lo consideriamo come stimatore della sua media,
che è una funzione del parametro.

\begin{example}
	Consideriamo la gaussiana a varianza fissata
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}} e^{-\frac12 (x-\mu)^2}.
	\end{equation*}
	L'informazione di Fisher\footnote{Vedi \autoref{th:fishgauss}.} per $N$ estrazioni è $N$.
	La media aritmetica è uno stimatore sufficiente e con bias nullo,
	e infatti la sua deviazione standard è $1/N$.
\end{example}

\begin{example}
	Consideriamo la gaussiana a media fissata
	\begin{equation*}
		p(x;\sigma)
		= \frac1{\sqrt{2\pi}\sigma} e^{-\frac12 \frac{x^2}{\sigma^2}}.
	\end{equation*}
	Lo scarto quadratico medio
	\begin{equation*}
		\hat\sigma^2(\mathbf x)
		\is \frac1N \sum_{i=1}^N x_i^2
	\end{equation*}
	è sufficiente e l'informazione di Fisher per $\sigma^2$ è $N/(2\sigma^4)$.
	Calcoliamo il valore atteso:
	\begin{align*}
		E[\hat\sigma^2]
		&= E \left[ \frac 1N \sum x^2 \right] = \\
		&= \frac 1N \sum E[x^2] = \\
		&= \sigma^2,
	\end{align*}
	quindi ha bias nullo.
	La varianza deve allora essere $1/I(\sigma^2)=2\sigma^4/N$, verifichiamolo:
	\begin{align*}
		E[\hat\sigma^4]
		&= E \left[ \frac1{N^2}\left(\sum_ix_i^2\right)^2 \right] = \\
		&= \frac1{N^2} \sum_{ij} E[x_i^2x_j^2] = \\
		&= \frac1{N^2} \left( \sum_{i=j} E[x_i^4] + \sum_{i\neq j} E[x_i^2]E[x_j^2] \right) = \\
		\Bigg[ E[x^4]
		&= \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, e^{-\frac12 x^2} x^4 = \\
		&= - \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, \dv{}{x} \left(e^{-\frac12x^2}\right) x^3
		= 3 \sigma^4 \Bigg] \\
		&= \frac1{N^2} \left( 3N + \frac{N(N-1)}2 \right) \sigma^4 = \\
		&= \frac{N+2}N \sigma^4,
	\end{align*}
	\begin{equation*}
		\var[\hat\sigma^2]
		= E[\hat\sigma^4] - E[\hat\sigma^2]^2
		= \frac{2\sigma^4}N.
	\end{equation*}
	Dunque $\hat\sigma^2$ è uno stimatore corretto ed efficiente di $\sigma^2$.
	Si può mostrare invece che $\hat\sigma$ come stimatore di $\sigma$ ha bias non nullo
	ma che non dipende da $\sigma$,
	quindi calcolato il bias lo si può sottrarre e per il \autoref{th:suffeff} $\hat\sigma$ rimane efficiente,
	quindi dall'informazione di Fisher e dal bias si ricava la varianza.
	\marginpar{Sul quaderno alla data 2017-11-23 ho scritto il conto del bias, magari metterlo in appendice.
	Forse il conto è più veloce dando per buona la distribuzione del $\chi^2$. \\
	\emph{Sul Del Prete a pagina 177 dice che non c'è uno stimatore efficiente di $\sigma$.}}
\end{example}
