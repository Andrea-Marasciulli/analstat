% Giacomo Petrillo
% lezione di Morello

\begin{example}
	Calcoliamo lo stimatore di massima likelihood per la poissoniana:
	\begin{align*}
		L \is \log p(\mathbf k;\mu)
		&= -\sum_i \log k_i! + \log\mu \sum_i k_i - N\mu \\
		0 = \pdv L\mu
		&= \sum_i k_i - N\mu \implies \\
		\implies \mu
		&= \bar k.
	\end{align*}
	Risulta la media aritmetica che già sappiamo essere uno stimatore unbiased e efficiente.
\end{example}

\begin{exercise}
	Calcolare lo stimatore di massima likelihood per $p$ della binomiale.
\end{exercise}

\begin{solution}
	\begin{align*}
		L \is \log p(\mathbf k;p)
		&= \sum_i\log\binom n{k_i} + \log p\sum_i k_i + \log(1-p) \sum_i(n-k_i) \\
		0 = \pdv Lp
		&= \frac{N\bar k}{p} - \frac{N(n-\bar k)}{1-p} \implies \\
		\implies p
		&= \frac{\bar k}n.
	\end{align*}
\end{solution}

\begin{example}[Media pesata]
	Consideriamo due variabili gaussiane indipendenti con la stessa media e calcoliamo lo stimatore di massima likelihood per la media:
	\begin{align*}
		L \is \log p(x_1,x_2;\mu)
		&= -\log(2\pi\sigma_1\sigma_2)
		-\frac12 \left(\frac{x_1-\mu}{\sigma_1}\right)^2
		- \frac12 \left(\frac{x_2-\mu}{\sigma_2}\right)^2 \\
		0 = \pdv L\mu
		&= \frac{x_1-\mu}{\sigma_1^2} + \frac{x_2-\mu}{\sigma_2^2} \implies \\
		\implies \mu
		&= \frac {\frac{x_1}{\sigma_1^2} + \frac{x_2}{\sigma_2^2}} {\frac1{\sigma_1^2} + \frac1{\sigma_2^2}}.
	\end{align*}
	Per $N$ variabili si generalizza immediatamente alla media pesata con $1/\sigma_i^2$:
	\begin{equation*}
		\hat\mu
		= \frac {\sum_i \frac{x_i}{\sigma_i^2}} {\sum_i \frac1{\sigma_i^2}}.
	\end{equation*}
	Calcoliamo la varianza:
	\begin{align*}
		\var[\hat\mu]
		&= \frac {\sum_i \frac{\sigma_i^2}{\sigma_i^4}} {\left(\sum_i \frac1{\sigma_i^2}\right)^2} = \\
		&= \frac1{\sum_i \frac1{\sigma_i^2}}.
	\end{align*}
\end{example}

\begin{example}
	Consideriamo $N$ estrazioni della distribuzione esponenziale
	\begin{equation*}
		p(t;\tau)
		= \frac1\tau e^{-t/\tau}
	\end{equation*}
	e calcoliamone lo stimatore di massima likelihood per $\tau$:
	\begin{align*}
		L \is \log p(\mathbf t;\tau)
		&= -N\log\tau - \frac{N\bar t}\tau \\
		0 = \pdv L\tau
		&= -\frac N\tau + \frac{N\bar t}{\tau^2} \implies \\
		\implies \tau
		&= \bar t.
	\end{align*}
	Le proprietà di $\hat\tau$ ci sono già note.
	Se vogliamo ricavare lo stimatore di massima likelihood per $\lambda=1/\tau$,
	essendo che il massimo non dipende dalla parametrizzazione,
	la trasformazione è banale:
	\begin{equation*}
		\hat\lambda = \frac1{\hat\tau} = \frac1{\bar t}.
	\end{equation*}
	Calcoliamo il bias di $\hat\lambda$.
	La distribuzione della somma di esponenziali\footnote{Vedi \autoref{th:salvexp}.} è
	\begin{equation*}
		p(s;\lambda)
		= \frac{\lambda^N}{(N-1)!}s^{N-1}e^{-\lambda s},
	\end{equation*}
	calcoliamo il valore atteso di $\hat\lambda$:
	\begin{align*}
		E \left[\frac1{\bar t}\right]
		&= N E \left[\frac1{\sum_i t_i}\right] = \\
		&= N \int_0^\infty \de s\, \frac{\lambda^N}{(N-1)!}s^{N-1}e^{-\lambda s} \frac 1s = \\
		&= \frac{N\lambda}{(N-1)!} \int_0^\infty \lambda\de s\, (\lambda s)^{N-2} e^{-\lambda s} = \\
		&= \frac{N\lambda}{(N-1)!} (N-2)! = \\
		&= \frac N{N-1} \lambda.
	\end{align*}
\end{example}

% la somma di esponenziali è già in un esercizio del 4 ottobre


