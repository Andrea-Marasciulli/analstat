% Giacomo Petrillo
% lezione di Morello

\begin{example}
	Calcoliamo lo stimatore di massima likelihood per la poissoniana:
	\begin{align*}
		L \is \log p(\mathbf k;\mu)
		&= -\sum_i \log k_i! + \log\mu \sum_i k_i - N\mu \\
		0 = \pdv L\mu
		&= \sum_i k_i - N\mu \implies \\
		\implies \mu
		&= \bar k.
	\end{align*}
	Risulta la media aritmetica che gi√† sappiamo essere uno stimatore unbiased e efficiente.
\end{example}

\begin{exercise}
	Calcolare lo stimatore di massima likelihood per $p$ della binomiale.
\end{exercise}

\begin{solution}
	\begin{align*}
		L \is \log p(\mathbf k;p)
		&= \sum_i\log\binom n{k_i} + \log p\sum_i k_i + \log(1-p) \sum_i(n-k_i) \\
		0 = \pdv Lp
		&= \frac{N\bar k}{p} - \frac{N(n-\bar k)}{1-p} \implies \\
		\implies p
		&= \frac{\bar k}n.
	\end{align*}
\end{solution}

\begin{example}[Media pesata]
	Consideriamo due variabili gaussiane indipendenti con la stessa media e calcoliamo lo stimatore di massima likelihood per la media:
	\begin{align*}
		L \is \log p(x_1,x_2;\mu)
		&= -\log(2\pi\sigma_1\sigma_2)
		-\frac12 \left(\frac{x_1-\mu}{\sigma_1}\right)^2
		- \frac12 \left(\frac{x_2-\mu}{\sigma_2}\right)^2 \\
		0 = \pdv L\mu
		&= \frac{x_1-\mu}{\sigma_1^2} + \frac{x_2-\mu}{\sigma_2^2} \implies \\
		\implies \mu
		&= \frac {\frac{x_1}{\sigma_1^2} + \frac{x_2}{\sigma_2^2}} {\frac1{\sigma_1^2} + \frac1{\sigma_2^2}}.
	\end{align*}
	Per $N$ variabili si generalizza immediatamente alla media pesata con $1/\sigma_i^2$:
	\begin{equation*}
		\hat\mu
		= \frac {\sum_i \frac{x_i}{\sigma_i^2}} {\sum_i \frac1{\sigma_i^2}}.
	\end{equation*}
	Calcoliamo la varianza:
	\begin{align*}
		\var[\hat\mu]
		&= \frac {\sum_i \frac{\sigma_i^2}{\sigma_i^4}} {\left(\sum_i \frac1{\sigma_i^2}\right)^2} = \\
		&= \frac1{\sum_i \frac1{\sigma_i^2}}.
	\end{align*}
\end{example}
