% Giacomo Petrillo
% lezione di Punzi

\begin{definition}[Matrice di covarianza]
	Date $n$ variabili $x_i$ si definisce \emph{matrice di covarianza}
	\begin{equation*}
		A_{ij} = \cov[x_i,x_j] \is E[(x_i-\avg{x_i})(x_j-\avg{x_j})].
	\end{equation*}
	Gli elementi diagonali sono le varianze e quelli fuori diagonale sono le covarianze.
\end{definition}

Data una statistica abbiamo visto come ricavarne esattamente la distribuzione;
può però essere utile ricavare solo alcuni momenti in modo approssimato.
Sia $\mathbf x\in\R^n$, sviluppiamo $s(\mathbf x)$ in serie intorno alla media:
\begin{align*}
	s(\mathbf x) &= s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) + {}\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} (x_i-\mu_i)(x_j-\mu_j) + {}\\
	&+ O(|\mathbf x - \boldsymbol\mu|^3).
\end{align*}
Calcoliamo la media e la varianza, ricordando che il valore atteso è lineare:
\begin{align*}
	E[s(\mathbf x)] &\approx s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} E[x_i - \mu_i] + {} & \Big\} = 0\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} E[(x_i-\mu_i)(x_j-\mu_j)] = \\
	&= s(\boldsymbol \mu) +
	\frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j];
\end{align*}
\begin{align*}
	\var[s(\mathbf x)] &= E[(s - \avg s)^2] \approx \\
	&\approx E\left[
	\sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) \cdot
	\sum_j \left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu} (x_j - \mu_j) \right] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	E[ (x_i - \mu_i) (x_j - \mu_j) ] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j].
\end{align*}
Nel caso che le $x_i$ siano indipendenti, questa approssimazione della varianza diventa
\begin{equation*}
	\var[s] \approx \sum_i
	\left( \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} \right)^2
	\var[x_i].
\end{equation*}
Probabilmente avrete già incontrato questa formula come ``formula di propagazione degli errori'';
sottolineiamo che è una del tutto generale approssimazione di un momento di una statistica e che non è detto sia sempre una buona approssimazione quando la usiamo per la propagazione degli errori (o, come preferiamo chiamarle, incertezze).

\begin{definition}[Funzione generatrice]
	Data la pdf $p(x)$, la sua \emph{funzione generatrice} è
		$M(t) = E[e^{tx}]$.
\end{definition}

La funzione generatrice è utile per calcolare i momenti, infatti:
\begin{align*}
	M^{(n)}(t) &=
	\frac{\partial^n}{\partial t^n} E[e^{tx}] =
	E \left[ \frac{\partial^n}{\partial t^n} e^{tx} \right] =
	E[x^n e^{tx}]
	\intertext{che per $t=0$ diventa}
	&= E[x^n] = \mu_n(0).
\end{align*}
In base al \autoref{th:pdfmom} segue che la generatrice determina completamente la pdf.

Per due variabili indipendenti $x$, $y$ vale che
\begin{equation*}
	M_{x+y}(t) =
	E[e^{t(x+y)}] =
	E[e^{tx} e^{ty}] =
	E[e^{tx}] E[e^{ty}] =
	M_x(t) M_y(t);
\end{equation*}
inoltre ovviamente $M_{\alpha x}(t) = M_x(\alpha t)$.

\begin{example}
	Consideriamo una distribuzione uniforme in $[0,m]$:
	\begin{equation*}
		p(x) = \begin{cases}
			\frac 1m & 0 \le x \le m \\
			0  & \text{altrimenti.}
		\end{cases}
	\end{equation*}
	Calcoliamo la generatrice:
	\begin{align*}
		M(t) &= 
		\int \de x\, p(x) e^{tx} =
		\int_0^m \de x\, \frac{e^{tx}}{m} =
		\frac 1m \left[ \frac{e^{tx}}{t} \right]_{t=0}^m = \\
		&= \frac{e^{mt} - 1}{mt}.
	\end{align*}
	Sviluppiamo $M$ in serie:
	\begin{align*}
		M(t) &=
		\frac{mt + \frac{(mt)^2}2 + \frac{(mt)^3}{3!} + \dotsb}{mt} = \\
		&= 1 + \frac{mt}2 + \frac{(mt)^2}{3!} + \dotsb = \\
		&= \mu_0 + \mu_1t + \frac{\mu_2}2t^2 + \dotsb  \\
		\implies &\begin{cases}
			\mu_0 = 1 \\
			\mu_1 = \frac m2 \\
			\mu_2 = \frac{m^2}3 \\
			\vdots
		\end{cases}
	\end{align*}
	Dai primi momenti possiamo calcolare direttamente la varianza:
	\begin{equation*}
		\sigma^2 = \mu_2 - \mu_1^2 = \frac{m^2}{12}.
	\end{equation*}
\end{example}

La distribuzione di una variabile che può assumere solo due valori si chiama \emph{distribuzione di Bernoulli}.
Fissiamo la notazione:
\begin{equation*}
	x \in \set{0,1}, \quad P(x) = \begin{cases}
		p & x = 1 \\
		1-p & x = 0.
	\end{cases}
\end{equation*}
Date $n$ variabili di Bernoulli indipendenti calcoliamo la probabilità che $k$ siano 1.
Il modo di disporre $k$ oggetti identici in $n$ posti è $\binom nk$, quindi
\begin{equation*}
	P(k;n,p) = \binom nk p^{k} (1-p)^{n-k};
\end{equation*}
questa distribuzione si chiama \emph{binomiale}.
Calcoliamone la generatrice:
\begin{align*}
	M_k(t) &=
	\sum_k e^{tk} P(k;n,p) =
	\sum_k \binom nk p^k e^{tk} (1-p)^{n-k} = \\
	&= (pe^t + 1-p)^n,
\end{align*}
ma in effetti la generatrice di una Bernoulli è
\begin{equation*}
	M_x(t) = pe^{1\cdot t} + (1-p)e^{0\cdot t} = pe^t + 1-p
\end{equation*}
e $k$ può essere scritto come $\sum_ix_i$, quindi deve essere $M_k = M_x^n$.

Calcoliamo ora la probabilità che in un intervallo di tempo $T$ si verifichino $k$ eventi se per il singolo evento è definita una probabilità per unità di tempo $\lambda$.
Dividiamo l'intervallo $T$ in $N$ sottointervalli uguali.
Nel limite $N\to\infty$, la probabilità che in un sottointervallo si verifichi un solo evento è
\begin{equation*}
	p \is \lambda \frac TN,
\end{equation*}
quindi la probabilità $p_q$ che in un sottointervallo si verifichino $q>0$ eventi è $p^q$,
\marginpar{invece dovrebbe essere $p^q/q!$}
mentre la probabilità che si verifichino 0 eventi è
\begin{equation*}
	p_0 = 1 - \sum_{q\ge1} p^q = 1 - \left(\frac1{1-p} - 1\right) = \frac{1-2p}{1-p}.
\end{equation*}
Sia $k_q$ il numero di sottointervalli in cui si verificano $q$ eventi,
se $k$ è il numero totale di eventi vale ovviamente
\begin{equation*}
	k = \sum_{q\ge1} q k_q, \quad k_0 = N - \sum_{q\ge 1} k_q \isis N - s.
\end{equation*}
La distribuzione dei $k_q$ è la multinomiale
\begin{align*}
	P(k_1,k_2,\dotsc)
	&= \frac{N!}{k_0!k_1!\dotsm} p_0^{k_0} p_1^{k_1} \dotsm = \\
	&= \frac{N!}{(N - s)!} \left(\frac{1-2p}{1-p}\right)^{N-s} \prod_{q\ge1} \frac{p^{qk_q}}{k_q!} = \\
	&= \frac{N!}{(N - s)!} \left(\frac{1-2\frac{\lambda T}{N}}{1-\frac{\lambda T}{N}}\right)^{N-s} \left(\frac{\lambda T}{N}\right)^k \prod_{q\ge1} \frac1{k_q!}.
\end{align*}
Facendo il limite per $N\to\infty$, risulta
\begin{equation*}
	P(k_1,k_2,\dotsc)
	= N^s e^{-\lambda T} \left(\frac{\lambda T}{N}\right)^k \prod_{q\ge1} \frac1{k_q!}
	= O\left(N^{\sum_{q\ge1} k_q - \sum_{q\ge1} qk_q}\right)
\end{equation*}
che non va a zero solo se $k_q=0$ per $q > 1$ ovvero se $k=k_1$,
quindi alla fine la probabilità di avere $k$ eventi è data semplicemente da
\begin{equation*}
	P(k) = e^{-\lambda T} (\lambda T)^k \frac1{k!};
\end{equation*}
definendo $\mu\is\lambda T$, abbiamo
\begin{equation*}
	P(k;\mu) = \frac{\mu^k}{k!} e^{-\mu}.
\end{equation*}
Questa distribuzione si chiama \emph{poissoniana}.

Nello stesso contesto in cui abbiamo ricavato la poissoniana,
calcoliamo la pdf del tempo che intercorre tra un istante fissato $t=0$ e l'evento successivo.
Sia $F(t)$ la cumulante, si ha
\begin{align*}
	1-F(t+\de t)
	&= P(\text{$0$ eventi in $(0,t+\de t)$}) = \\
	&= P(\text{$0$ eventi in $(0,t)$}) \cdot P(\text{$0$ eventi in $(t,t+\de t)$}) = \\
	&= (1-F(t)) (1-\lambda\de t) = \\
	&= 1 - F(t) - \lambda(1-F(t))\de t \rightarrow \\
	&\rightarrow \frac{\de F}{\de t} = \lambda(1-F(t)) \rightarrow \\
	&\rightarrow F(t) = 1 - e^{-\lambda t} \rightarrow \\
	&\rightarrow p(t) = \frac{\de F}{\de t} = \lambda e^{-\lambda t}.
\end{align*}
Questa distribuzione si chiama \emph{esponenziale}.

Notiamo che la distribuzione esponenziale non ammette generatrice.
Esiste però~la
\begin{definition}[Funzione caratteristica]
	La \emph{funzione caratteristica} è la trasformata di Fourier della pdf:
	\begin{equation*}
		\phi(t) \is E[e^{itx}].
	\end{equation*}
\end{definition}
Dalla discussione sulle generatrici abbiamo immediatamente che
\begin{equation*}
	\phi_{x+y} = \phi_x\phi_y, \quad \mu_n = i^{-n} \phi^{(n)}(0).
\end{equation*}

\begin{fact}
	Consideriamo una successione $F_n$ di cumulanti e la successione $\phi_n$ delle corrispondenti caratteristiche.
	Se esiste il limite puntuale di $\phi_n$, cioè
	\begin{equation*}
		\forall t \quad \exists \lim_{n\to\infty} \phi_n(t) \isis \phi(t),
	\end{equation*}
	e il limite $\phi$ è continuo in 0, allora
	esiste una $F$ che è il limite puntuale di $F_n$ dove è continua, cioè
	\begin{equation*}
		\exists F \quad \forall x: \text{$F$ continua in $x$} \implies
		\lim_{n\to\infty} F_n(x) = F(x).
	\end{equation*}
\end{fact}
