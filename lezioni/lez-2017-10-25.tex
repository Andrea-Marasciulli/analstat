% Giacomo Petrillo
% non c'ero a lezione quindi uso gli appunti di Francesco Serra
% lezione di Punzi

Consideriamo la statistica $s_k(x)\is x^k$.
Il valore atteso è per definizione il momento $k$-esimo
\marginpar{quale lettera usavamo per i momenti?}%
$E[s_k]=\mu_k$,
quindi $s_k$ è uno stimatore con bias nullo di $\mu_k$.
Verifichiamo che sia consistente;
per $N$ estrazioni lo estendiamo alla media aritmetica (che lascia invariato il bias):
\begin{align*}
	s_k(\mathbf x)
	\is \frac{\sum_i x_i^k}N, \quad
	E[s_k(\mathbf x)]
	= \frac1N \sum_i E[x_i^k]
	= \mu_k,
\end{align*}
e calcoliamo la varianza:
\begin{align*}
	E[s_k^2]
	&= \frac1{N^2} E \left[ \sum_{ij} x_i^k x_j^k \right] = \\
	&= \frac1{N^2} \sum_{ij} E[x_i^k x_j^k] = \\
	&= \frac1{N^2} \left( \sum_{i=j} E[x_i^{2k}] + \sum_{i\neq j} E[x_i^k] E[x_j^k] \right) = \\
	&= \frac1{N^2} (N\mu_{2k} + N(N-1)\mu_k^2), \\
	\var[s_k]
	&= E[s_k^2] - E[s_k]^2 = \\
	&= \frac{\mu_{2k} - \mu_k^2}N
	\propto \frac1N.
\end{align*}
I momenti sono quindi un modo generale di parametrizzare una distribuzione in modo che esista uno stimatore corretto, consistente e semplice da calcolare.
Tuttavia tipicamente il problema specifico fisserà quali sono i parametri che vogliamo stimare.

Consideriamo un modello $p(x;\theta)$.
Fissiamo un ``valore vero'' $\theta_0$ per il parametro.
\marginpar{Probabilmente prima che qui bisogna dire che per valore vero intendiamo quello su cui implicitamente calcoliamo la probabilità es. nei valori attesi.}%
Sia $f(x,\theta)$ tale che la funzione
\begin{equation*}
	h(\theta)\is E[f(x,\theta)]
\end{equation*}
si annulla solo in $\theta_0$.
Consideriamo $N$ estrazioni.
Per il \autoref{th:grossnum},
la media aritmetica $\sum_i f(x_i,\theta)/N$ converge in probabilità a $h(\theta)$.
Sia $s_N(\mathbf x)$ una statistica tale che
\begin{equation*}
	\forall N \,\forall\mathbf x: \frac{\sum_i f(x_i,s_N(\mathbf x))}N = 0,
\end{equation*}
allora $s_N$ converge in probabilità a $\theta_0$.
\marginpar{Ipotesi tecniche su $f$?}%
Ora esibiamo una $f$ e una $s_N$ con queste proprietà.
\begin{align*}
	1
	&= \int\de x\, p(x;\theta) \implies \\
	\implies 0
	&= \pdv{}{\theta} \int\de x\, p(x;\theta) = \\
	&= \int\de x\, \pdv{}{\theta} p(x;\theta) = \\
	&= \int\de x\, p(x;\theta) \pdv{}{\theta} \log p(x;\theta) \implies \\
	\implies 0
	&= E \left[ \pdv{}{\theta} \log p(x;\theta) \big|_{\theta_0} \right],
\end{align*}
quindi $\partial_\theta \log p(x;\theta)$ ha le proprietà richieste per $f$,
assumendo che lo zero del valore atteso sia unico.
Definiamo
\begin{equation*}
	K_{\mathbf x}(\theta)
	\is \frac{\sum_i \pdv{}{\theta} \log p(x;\theta)} N.
\end{equation*}
Sia $\hat\theta$ un punto stazionario di $p(\mathbf x,\theta)$.
Allora
\begin{align*}
	0
	&= \pdv{}{\theta} \log p(\mathbf x;\theta) \big|_{\hat\theta} = \\
	&= \pdv{}{\theta} \sum_i \log p(x_i;\theta) \big|_{\hat\theta} \implies \\
	\implies 0
	&= K_{\mathbf x}(\hat\theta),
\end{align*}
quindi $\hat\theta$ soddisfa le proprietà richieste per $s_N$.
Supponiamo ora che il valore atteso di
$\partial_\theta \log p(x;\theta)$
abbia più di uno zero.
Tuttavia
\begin{align*}
	E \left[ K_{\mathbf x}'(\theta_0) \right]
	&= E \left[ \Pdv{}{\theta} \log p(\mathbf x;\theta) \big|_{\theta_0} \right] = \\
	&= -I_{\mathbf x}(\theta_0)
	\le 0,
\end{align*}
quindi se $\hat\theta$ è una statistica che fa convergere $K_{\mathbf x}(\hat\theta)$ a $0$,
deve essere necessariamente un massimo di $\log p(\mathbf x;\theta)$.
\marginpar{Non ho capito bene.}%
Allora se il massimo è unico, essendo un punto stazionario,
converge a $\theta_0$.
Per la convergenza è sufficiente che il massimo sia asintoticamente unico.
Abbiamo dunque lo \emph{stimatore di massima likelihood},
che è consistente.

Consideriamo l'espansione in serie di $K$:
\begin{align*}
	0 = K(\hat\theta)
	&= K(\theta_0) + K'(\theta_0)(\hat\theta-\theta_0) + O\big((\hat\theta-\theta_0)^2\big) \implies \\
	\implies \hat\theta
	&= \theta_0 + \frac{K(\hat\theta)}{-K'(\theta_0)} + O\big((\hat\theta-\theta_0)^2\big)
\end{align*}
Per il \autoref{th:limitecentrale} (limite centrale),
per $N\to\infty$
la distribuzione di $K(\hat\theta)$ converge a una gaussiana
con media 0 e varianza $I/N$.
Inoltre abbiamo visto che $K'(\theta_0)$ converge in probabilità a $-I$,
quindi la distribuzione di $\hat\theta$ converge a una gaussiana
con media $\theta_0$ e varianza $1/(IN)$.
Il termine all'ordine quadratico converge a un ordine della varianza,
quindi il bias è $O(1/N)$.
Notiamo che il rapporto tra il bias e la deviazione standard va a zero come $1/\sqrt{N}$.
